Number of parameters (billion), Attention heads, Hidden size, Number of layers, Tensor model-parallel size, Pipeline model-parallel size, Number of GPUs, Batch size, Achieved teraFLOP/s per GPU), Percentage of theoretical peak FLOP/s, Achieve aggregate petaFLOP/s
1.7, 24, 2304, 24, 1, 1, 32, 512, 137, 44%, 4.4
3.6, 32, 3072, 30, 2, 1, 64, 512, 138, 44%, 8.8
7.5, 32, 4096, 36, 4, 1, 128, 512, 142, 46%, 18.2
18.4, 48, 6144, 40, 8, 1, 256, 1024, 135, 43%, 34.6
39.1, 64, 8192, 48, 8, 2, 512, 1536, 138, 44%, 70.8
76.1, 80, 10240, 60, 8, 4, 1024, 1792, 140, 45%, 143.8
145.6, 96, 12288, 80, 8, 8, 1536, 2304, 148, 47%, 227.1
310.1, 128, 16384, 96, 8, 16, 1920, 2160, 155, 50%, 297.4
529.6, 128, 20480, 105, 8, 35, 2520, 2520, 163, 52%, 410.2
1008, 160, 25600, 128, 8, 64, 3072, 3072, 163, 52%, 502