{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "978354b4",
      "metadata": {
        "id": "978354b4"
      },
      "source": [
        "# Estimation of CO2 equivalent emissions of tranformer based large language models\n",
        "\n",
        "We estimate the equivalent carbon emission for transformer based LLMs. Following models are used:\n",
        "- GPT\n",
        "- BERT\n",
        "- T5\n",
        "\n",
        "We focus on the total training co2eq estimation since it is more deterministic compared to inference.\n",
        "\n",
        "The equivalent training carbon footprint depends on:\n",
        "- Total Training Time\n",
        "- Number of GPUs\n",
        "- Thermal Design Power(TDP) of GPUs\n",
        "- Regional carbon equivalent emissions\n",
        "- Power Usage Effectiveness(PUE)\n",
        "\n",
        "We estimate throughput to find total train time and carbon emission. A linear regression using a 2nd order polynomial is fit on the throughput scaling data presented in the paper [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473). The final curve returns throughput FLOPs per gpu given P number of parameters. The paper uses autoregressive transformer models like GPT-3 for its study.\n",
        "\n",
        "The empirical data presented in the Megatron LM paper for throughput scaling uses specific compute and communication optimizations for workload distribution. This impacts the parallelization degrees i.e. pipe, tensor, and data parallel. Data used in this paper is therefore constrained by the specific hardware of the experimental setup(**DGX A100 NVIDIA nodes**). To extend this throughput estimation to NVIDIA 32GB V100 GPUs we use their ratio of relative performance([source](https://lambdalabs.com/gpu-benchmarks)). Using this ratio alone is a rough extension though since it does not consider the impact of p,t,d scaling using Megatron framework for A100 GPU reported throughput values.\n",
        "\n",
        "Naturally, the total workload is split among all workers so we have $p.t.d=n$ where n is the total number of GPUs. We assume that reported throughput in the Megatron LM paper is an upper bound for GPT like models of a given parameter size.\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "### Training time estimation\n",
        "\n",
        "For an approximate training time calculation, we need to estimate the following:\n",
        "- Total train FLOPs required by the model\n",
        "- Benchmark of single GPU FLOPs\n",
        "- Percent of peak device throughput as estimated using the regression equation\n",
        "\n",
        "This gives the train time as: $t_{train} = \\frac{\\text{Total Train FLOPs}}{\\text{(Benchmark FLOPs per GPU)}*\\text{Percent Utilization}*\\text{#GPUs}}$\n",
        "\n",
        "To calculate the total compute FLOPs during training for different large transformer models we refer to the paper [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165). The following table gives FLOPs per parameter per token for different model types. This factor is multiplied with total tokens and parameters. We do not consider attention operation FLOPs since these are $<10\\%$ of overall.\n",
        "\n",
        "![flop_count.png](flop_count.png)\n",
        "\n",
        "The total train compute can be defined as: $train_{compute} = T.P.f_p$ where <br>\n",
        "- $T$: Total Training Tokens\n",
        "- $P$: Total Parameters\n",
        "- $f_p$: FLOPs required per token per parameter\n",
        "    - BERT/GPT: 6 (100% of parameters active for each token)\n",
        "    - T5: 3 (50% of parameters active for each token)    \n",
        "    \n",
        "Megatron scaling has been used for each of GPT, BERT, and T5 models leading to a safe assumtion of throughput behavior showing similar trends across each of the model type given specific hardware configuration.\n",
        "\n",
        "We scale throughput to find percent of peak utilization. Peak FLOPs across GPU types are taken from NVIDIA specsheet:\n",
        "- A100 40/80GB: 312 TFLOPs\n",
        "- V100 32GB: 125 TFLOPs\n",
        "\n",
        "### CO2e calculation\n",
        "\n",
        "CO2e i.e. equivalent carbon emission is the product of the following: <br>\n",
        "- Total train time\n",
        "- Thermal Design Power(TDP) of GPU\n",
        "- Regional carbon equivalent emissions\n",
        "    - US national average for 2018 is 1.58\n",
        "    - GPT-3 training time PUE is 0.429\n",
        "- Power Usage Effectiveness(PUE)\n",
        "    - OpenAI reported PUE for GPT-3 training is 1.1\n",
        "\n",
        "\n",
        "Gross CO2e emisssion can hence be estimated as <br>\n",
        "*KWh = Hours to train × Number of Processors × Average Power per Processor × PUE ÷ 1000* <br>\n",
        "*tCO2e = KWh × kg CO2e per KWh ÷ 1000* <br>\n",
        "\n",
        "Net CO2e emission factors into account carbon offset by multiple methods such as buying carbon credits, using renewable grid and others. Net carbon equivalent is not calculated here.\n",
        "\n",
        "---\n",
        "\n",
        "<mark>*Assumption*</mark> Mem capacity of a 80GB GPU:  2.4B parameter model. Other gpu parameter capacity is based on this unit<br>\n",
        "\n",
        "Let\n",
        "- Total number of parameters: $P$ <br>\n",
        "- GPU type: V100/A100 <br>\n",
        "- GPU memory: $gpu_{mem}$ <br>\n",
        "- Number of GPUs in a single node: $node_{size}$ (restricted to 1,2,4,8)<br>\n",
        "- Parameter capacity of a single GPU: $gpu_{cap}$ <br>\n",
        "- Parameter capacity of a single node: $node_{cap}$ <br>\n",
        "- Estimated total number of GPUs: $n$ <br>\n",
        "- Estimated Batch Size: $B$ <br>\n",
        "- Estimated tensor size: $tensor$ <br>\n",
        "- Estimated pipeline size: $pipe$ <br>\n",
        "- Estimated data size: $data$ <br>\n",
        "- Estimated throughput: $X$ <br>\n",
        "- A100:V100 ratio: $r$ <br>\n",
        "- FLOPs per parameter per token: $flop_{token}$ <br>\n",
        "- FLOP benchmark GPU: $flop_{bench}$ <br>\n",
        "- Total training tokens: $T$ <br>\n",
        "- Total train compute: $C_{train}$\n",
        "- End-to-end training time: $t_{train}$ <br>\n",
        "- Gross CO2e emission estimate: $co2e_{gross}$\n",
        "\n",
        "\n",
        "**__Algorithm__** <br>\n",
        "1. Calculate total parameters $ P = 12lh^2(1+\\frac{13}{12h} + \\frac{V+s}{12lh})$ <br>\n",
        "2. Use regression coefficients for estimating number of GPUs $n$ <br>\n",
        "3. Use regression coefficients for estimating batch size $B$  <br>\n",
        "4. Calculate parameter capacity of a single node $node_{cap} = node_{size}*gpu_{cap}$ <br>\n",
        "5. if total parameters($P$) < parameter cap for a single node($node_{cap}$) <br>\n",
        "    5.1 Set pipeline size $pipe=1$ and tensor size $tensor = \\lceil \\frac{P}{gpu_{cap}} \\rceil$ <br>\n",
        "    5.2 else set pipeline size $pipe=\\lceil \\frac{P}{node_{cap}} \\rceil$ and tensor size $tensor = node_{size}$\n",
        "6. Use regression coefficients and p,t,d for estimating throughput $X$ and peak utilization given A100 nodes<br>\n",
        "    6.1 Use relative performance ratio to scale to V100 GPU type\n",
        "7. Calculate the total training compute $C_{train} = flop_{token} * T * P$ <br>\n",
        "8. Calculate total training time $t_{train} = \\frac{C_{train}}{n*\\text{(percent of peak)}*flop_{bench}}$ <br>\n",
        "9. Calculate gross CO2e estimate as <br>\n",
        "KWh = Hours to train × Number of Processors × Average Power per Processor × PUE ÷ 1000 <br>\n",
        "tCO2e = KWh × kg CO2e per KWh ÷ 1000 <br>\n",
        "$\\implies \\mathbf{co2e_{gross} = n*t_{train}*\\text{GPU TDP}*\\text{PUE}*\\text{Datacenter gross CO2 e /KWh}}$\n",
        "\n",
        "---\n",
        "\n",
        "## Implementation\n",
        "\n",
        "Estimated regression coefficients used for polynomial fit $ \\mathbf{y = ax^2 + bx + c} $\n",
        "- Tensor model throughput: $$a= -8.82079068\\times 10^{-20},  b= 1.68591116\\times 10^{-09},  c= 1.33954735\\times 10^{+02}$$\n",
        "- Pipeline model throughput: $$a= -5.60233749\\times 10^{-23},  b= 8.45435587\\times 10^{-11},  c= 1.34546129\\times 10^{+02}$$\n",
        "- Total number of GPUs: $$a= -2.12910565\\times 10^{-21},  b= 4.39684339\\times 10^{-09},  c=7.99173057\\times 10^{+02}$$\n",
        "- Batch Size: $$a = -4.29439186\\times 10^{-01},  b= 5.21376002\\times 10^{+01},  c= 1.43737095\\times 10^{+03}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f36ee0b0",
      "metadata": {
        "id": "f36ee0b0"
      },
      "source": [
        "### Scaled throughput calculation\n",
        "\n",
        "Data used for regression uses 8x 80GB A100 NVLink nodes for the reported throughput. From observed data, the ratio of relative performance between 8X 32GB V100 vs 8X A100 is $r = \\frac{7.76}{33.46} = 0.23$. This reported factor considers large transformer models and fp-16 mixed precision training used in LLMs.\n",
        "\n",
        "We use this factor of 0.23 to scale reported throughput for A100 GPU in original paper to 32GB V100 GPU used by Patterson et al to report their estimated CO2e calculation. This scaling is done by sclaing the peak percent performance of V100 GPU by the same amount as A100 reported throughput after reducing by a factor 0.23. We use this percent of peak for V100 GPUs to calculate throughput used in all calculations for V100.\n",
        "\n",
        "Example:\n",
        "- Let model be GPT with parameter size 100B\n",
        "- Let estimated throughput($X$) for the 100B model be 140 TFLOPs\n",
        "- New throughput $X_{new} = X(1 - r)$\n",
        "- $X_{new}$ percent peak: $p_{new} = \\frac{X_{new}}{312}$\n",
        "- Hence estimated throughput for V100: $p_{new}*125$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8116ff81",
      "metadata": {
        "id": "8116ff81"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eca8db46",
      "metadata": {
        "id": "eca8db46"
      },
      "source": [
        "**Total Parameters and GPU details**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5bea7179",
      "metadata": {
        "id": "5bea7179"
      },
      "outputs": [],
      "source": [
        "V = 51200\n",
        "s = 2048\n",
        "h = 2304\n",
        "a = 24\n",
        "l = 24\n",
        "P = 12*l*(h**2)*(1 + (13/(12*h)) + ((V+s)/(12*l*h)))\n",
        "P_user = 175e9 # explicitly defined number of parameters\n",
        "gpu_map = {\n",
        "    'A100': {'tdp': 0.4, 'peak': 312},\n",
        "    'V100': {'tdp': 0.3, 'peak': 125}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "808bd6d1",
      "metadata": {
        "id": "808bd6d1"
      },
      "source": [
        "**Model Training and regression functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f807a049",
      "metadata": {
        "id": "f807a049"
      },
      "outputs": [],
      "source": [
        "model_type = 'GPT' # GPT, BERT, T5\n",
        "tokens = 300e9 # training tokens\n",
        "\n",
        "region_co2 = 0.429 #OPENAI GPT 3\n",
        "pue = 1.1 #reported by NVIDIA for Azure datacenter\n",
        "\n",
        "# regression coefficients basis observed Megatron scaling for throughput\n",
        "coeff_tensor = np.array([-8.82079068e-20,  1.68591116e-09,  1.33954735e+02])\n",
        "coeff_pipe = np.array([-5.60233749e-23,  8.45435587e-11,  1.34546129e+02])\n",
        "coeff_gpu = np.array([-2.12910565e-21,  4.39684339e-09,  7.99173057e+02])\n",
        "coeff_batch = np.array([-4.29439186e-01,  5.21376002e+01,  1.43737095e+03])\n",
        "\n",
        "func_tensor = np.poly1d(coeff_tensor)\n",
        "func_pipe = np.poly1d(coeff_pipe)\n",
        "func_gpu = np.poly1d(coeff_gpu)\n",
        "func_batch = np.poly1d(coeff_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c051cd21",
      "metadata": {
        "id": "c051cd21"
      },
      "source": [
        "**Funtion definition for parallel strategy, estimated throughput, end-to-end train time, gross co2e**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c3b5b61f",
      "metadata": {
        "id": "c3b5b61f"
      },
      "outputs": [],
      "source": [
        "def get_ptd(P, node_size, gpu_cap, gpu_type, gpu_mem):\n",
        "    p_b = P/1e9\n",
        "\n",
        "    # model parallel size\n",
        "    if p_b < node_size*gpu_cap:\n",
        "        p_size = 1\n",
        "        t_size = int(np.ceil(p_b/gpu_cap))\n",
        "    else:\n",
        "        t_size = node_size\n",
        "        p_size = int(np.ceil(p_b/(node_size*gpu_cap)))\n",
        "\n",
        "    model_size = t_size * p_size\n",
        "\n",
        "    # number of gpus estimate\n",
        "    num_gpu = np.round(func_gpu(P)/model_size)*model_size\n",
        "    if 'V100' in gpu_type:\n",
        "        num_gpu = np.round(num_gpu * 2.5)\n",
        "\n",
        "    if gpu_mem == 40:\n",
        "        num_gpu *= 2\n",
        "\n",
        "    d_size = num_gpu/model_size\n",
        "    #estimated batch size\n",
        "    if p_size == 1:\n",
        "        batch_size = 512\n",
        "    else:\n",
        "        batch_size = np.round(func_batch(p_size)/8)*8\n",
        "        if batch_size < num_gpu:\n",
        "            batch_size = num_gpu\n",
        "\n",
        "    return p_size, t_size, d_size, num_gpu, batch_size\n",
        "\n",
        "def get_throughput(t_size, p_size, node_size, P, gpu_type, rel_thru):\n",
        "    #intra model condition\n",
        "    if (t_size <= node_size and p_size == 1):\n",
        "        X = func_tensor(P)\n",
        "    # inter model\n",
        "    else:\n",
        "        X = func_pipe(P)\n",
        "\n",
        "    if 'V100' in gpu_type:\n",
        "        X_new = X -  X*rel_thru\n",
        "        peak_new = X_new /312\n",
        "        X_scaled = peak_new*125\n",
        "    else:\n",
        "        X_scaled = X\n",
        "\n",
        "    return X_scaled\n",
        "\n",
        "def get_train_time(model_type, tokens, P, n, X):\n",
        "    flop_per_parameter = 6\n",
        "    if 'T5' in model_type:\n",
        "        flop_per_parameter = 3\n",
        "\n",
        "    total_compute = P*tokens*flop_per_parameter\n",
        "    total_compute_per_sec = n*X*1e12\n",
        "\n",
        "    train_sec = total_compute / total_compute_per_sec\n",
        "\n",
        "    return train_sec, total_compute\n",
        "\n",
        "def get_co2e(gpu_tdp, train_time, region_co2, pue, n):\n",
        "    co2_gpu = gpu_tdp * train_time * region_co2 * pue\n",
        "    co2_gross = co2_gpu*n\n",
        "    return co2_gross"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c47f921",
      "metadata": {
        "id": "9c47f921"
      },
      "source": [
        "### Evaluation on 175B GPT-3 Parameter model\n",
        "\n",
        "We calculate the total co2e for GPT_Large(175B). We observe the difference between:\n",
        "- 80 GB A100 for NVLink\n",
        "- 32 GB V100 for NVLink\n",
        "\n",
        "There is no throughput scaling study for V100 GPUs for large transformer models hence we are using a throughput performance ratio of A100 vs V100 to estimate percent peak for V100 given this ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d38f536e",
      "metadata": {
        "id": "d38f536e"
      },
      "source": [
        "**Results for 8x 80GB A100 nodes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "974c852a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "974c852a",
        "outputId": "2a5a0216-445c-40f5-d1a2-72f36c8f2d55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated Number of GPU: 1520.0 \n",
            "P,T,D : 10, 8, 19.0 \n",
            "Estimated Batch Size: 1912.0 \n",
            "Total Compute Required: 3.150000e+23 FLOPs \n",
            "Estimated throughput: 147.63 TFLOPs \n",
            "Percent peak: 47.32 % \n",
            "Total Train Time: 17.0 days \n",
            "Gross CO2e: 111.90 tCO2e\n"
          ]
        }
      ],
      "source": [
        "node_size = 8\n",
        "gpu_type = 'A100' # A100 or V100\n",
        "gpu_mem = 80 # 40,32(for V100)\n",
        "gpu_cap = 0.03*gpu_mem\n",
        "node_cap = node_size*gpu_cap\n",
        "gpu_tdp = gpu_map[gpu_type]['tdp']\n",
        "gpu_peak = gpu_map[gpu_type]['peak']\n",
        "\n",
        "#relative throughput speedup ratio for 8X V100 vs A100 throughput\n",
        "rel_thru = 7.76/33.46\n",
        "\n",
        "p_size, t_size, d_size, num_gpu, batch_size = get_ptd(P_user, node_size, gpu_cap, gpu_type, gpu_mem)\n",
        "\n",
        "X = get_throughput(t_size, p_size, node_size, P_user, gpu_type, rel_thru)\n",
        "\n",
        "train_sec, total_compute  = get_train_time(model_type, tokens, P_user, num_gpu, X)\n",
        "train_hour = np.round(train_sec/3600)\n",
        "train_day = np.ceil(train_sec/86400)\n",
        "\n",
        "\n",
        "co2e_gross = get_co2e(gpu_tdp, train_hour, region_co2, pue, num_gpu)\n",
        "\n",
        "print('Estimated Number of GPU: {} \\n\\\n",
        "P,T,D : {}, {}, {} \\n\\\n",
        "Estimated Batch Size: {} \\n\\\n",
        "Total Compute Required: {:e} FLOPs \\n\\\n",
        "Estimated throughput: {:.2f} TFLOPs \\n\\\n",
        "Percent peak: {:.2f} % \\n\\\n",
        "Total Train Time: {} days \\n\\\n",
        "Gross CO2e: {:.2f} tCO2e'.format(num_gpu, p_size, t_size, d_size, batch_size, total_compute, X, \\\n",
        "                                 (X/gpu_peak)*100, train_day, co2e_gross/1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d5c3c4c",
      "metadata": {
        "id": "0d5c3c4c"
      },
      "source": [
        "**Results for 8x 32GB V100 nodes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2a8755b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a8755b9",
        "outputId": "f5b2354d-ab7e-4683-e7c9-7d3238543109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated Number of GPU: 3680.0 \n",
            "P,T,D : 23, 8, 20.0 \n",
            "Estimated Batch Size: 3680.0 \n",
            "Total Compute Required: 3.150000e+23 FLOPs \n",
            "Estimated throughput: 45.43 TFLOPs \n",
            "Percent peak: 36.34 % \n",
            "Total Train Time: 22.0 days \n",
            "Gross CO2e: 272.47 tCO2e\n"
          ]
        }
      ],
      "source": [
        "node_size = 8\n",
        "gpu_type = 'V100' # A100 or V100\n",
        "gpu_mem = 32 # 40,32(for V100)\n",
        "gpu_cap = 0.03*gpu_mem\n",
        "node_cap = node_size*gpu_cap\n",
        "gpu_tdp = gpu_map[gpu_type]['tdp']\n",
        "gpu_peak = gpu_map[gpu_type]['peak']\n",
        "\n",
        "#relative throughput speedup ratio for 8X V100 vs A100 throughput\n",
        "rel_thru = 7.76/33.46\n",
        "\n",
        "p_size, t_size, d_size, num_gpu, batch_size = get_ptd(P_user, node_size, gpu_cap, gpu_type, gpu_mem)\n",
        "\n",
        "X = get_throughput(t_size, p_size, node_size, P_user, gpu_type, rel_thru)\n",
        "\n",
        "train_sec, total_compute  = get_train_time(model_type, tokens, P_user, num_gpu, X)\n",
        "train_hour = np.round(train_sec/3600)\n",
        "train_day = np.ceil(train_sec/86400)\n",
        "\n",
        "\n",
        "co2e_gross = get_co2e(gpu_tdp, train_hour, region_co2, pue, num_gpu)\n",
        "\n",
        "print('Estimated Number of GPU: {} \\n\\\n",
        "P,T,D : {}, {}, {} \\n\\\n",
        "Estimated Batch Size: {} \\n\\\n",
        "Total Compute Required: {:e} FLOPs \\n\\\n",
        "Estimated throughput: {:.2f} TFLOPs \\n\\\n",
        "Percent peak: {:.2f} % \\n\\\n",
        "Total Train Time: {} days \\n\\\n",
        "Gross CO2e: {:.2f} tCO2e'.format(num_gpu, p_size, t_size, d_size, batch_size, total_compute, X, \\\n",
        "                                 (X/gpu_peak)*100, train_day, co2e_gross/1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "184890db",
      "metadata": {
        "id": "184890db"
      },
      "source": [
        "### Discussion on variation observed with total CO2e estimated for GPT(Large) model discussed in Patterson et al\n",
        "\n",
        "The paper by David Patterson et al on [Carbon Emissions and Large Neural Network Training](https://arxiv.org/abs/2104.10350) calculates equivalent carbon for different model types. The results observed on different models across different hardware is given below:\n",
        "\n",
        "![co2e_results](co2e_results.png)\n",
        "\n",
        "From the above data throughput used for this setup of 10,000 GPUs is <mark>24.6 TFLOPs (19.7% of peak)</mark>. This reported throughput does not uses Megatron to find optimized parallel dimensions of a given model size for workload distribution. Hence a valid assumption can be made regarding expected higher values of throughput when run with V100 GPU using P,T,D parallel sizes based on obervations made in Megatron LM.\n",
        "\n",
        "We estimate throughput per GPU for 3680 V100 GPUs as $45.43 \\text{TFLOPs} (36.34\\% of peak)$. This 17% increase from the reported throughput by Google for estimating CO2e leads to <mark>-51%</mark> lesser CO2e estimated from the regression equations versus the reported values in Patterson et al. If the same throughput values as used by Patterson et al are used for the estimated model configurations of 3680 GPUs with a training time of 22 days we only have a $-9\\%$ variation with the reported throughput in Patterson. Hence we find that this throughput difference is the main driver behind the large difference of $-57\\%$.\n",
        "\n",
        "### Result for parameters from 1B to 1000B\n",
        "\n",
        "Below graph shows the estimated throughput using the fit regression curve for GPT/BERT like models with parameters ranging from 1B to 1000B\n",
        "\n",
        "![a100_thru](a100_thru.png)\n",
        "![v100_thru](v100_thru.png)\n",
        "\n",
        "As expected, the estimated throughput follows the same trend as observed in Megatron paper."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16a4c740",
      "metadata": {
        "id": "16a4c740"
      },
      "source": [
        "#### Appendix\n",
        "**Function for batch run**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0d2f6555",
      "metadata": {
        "id": "0d2f6555"
      },
      "outputs": [],
      "source": [
        "def run(P_sizes, gpu_type, node_size, gpu_mem, rel_thru):\n",
        "    gpu_cap = 0.03*gpu_mem\n",
        "    node_cap = node_size*gpu_cap\n",
        "    gpu_tdp = gpu_map[gpu_type]['tdp']\n",
        "    gpu_peak = gpu_map[gpu_type]['peak']\n",
        "\n",
        "    #relative throughput speedup ratio for 8X V100 vs A100 throughput\n",
        "    # rel_thru = 7.76/33.46\n",
        "    result = list()\n",
        "    for P_size in P_sizes:\n",
        "        p_size, t_size, d_size, num_gpu, batch_size = get_ptd(P_size, node_size, gpu_cap, gpu_type, gpu_mem)\n",
        "\n",
        "        X = get_throughput(t_size, p_size, node_size, P_size, gpu_type, rel_thru)\n",
        "\n",
        "        train_sec, total_compute  = get_train_time(model_type, tokens, P_size, num_gpu, X)\n",
        "        train_hour = np.round(train_sec/3600)\n",
        "        train_day = np.ceil(train_sec/86400)\n",
        "        co2e_gross = get_co2e(gpu_tdp, train_hour, region_co2, pue, num_gpu)\n",
        "\n",
        "        r = (P_size, num_gpu, p_size, t_size, d_size, batch_size, total_compute, X, \\\n",
        "                                     (X/gpu_peak)*100, train_day, co2e_gross/1000)\n",
        "\n",
        "        result.append(r)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5001e2af",
      "metadata": {
        "id": "5001e2af"
      },
      "outputs": [],
      "source": [
        "pr_list = [1.7e9, 3.6e9, 7.5e9, 18.4e9, 39.1e9, 76.1e9, 145.6e9, 310.1e9, 529.6e9, 1008e9]\n",
        "node_size = 8\n",
        "gpu_type = 'A100' # A100 or V100\n",
        "gpu_mem = 80 # 40,32(for V100)\n",
        "rel_thru = 7.76/33.46\n",
        "a100_results = run(pr_list, gpu_type, node_size, gpu_mem, rel_thru)\n",
        "a100_X = []\n",
        "for el in a100_results:\n",
        "    a100_X.append(el[7])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "94216e51",
      "metadata": {
        "id": "94216e51"
      },
      "outputs": [],
      "source": [
        "pr_list = [1.7e9, 3.6e9, 7.5e9, 18.4e9, 39.1e9, 76.1e9, 145.6e9, 310.1e9, 529.6e9, 1008e9]\n",
        "node_size = 8\n",
        "gpu_type = 'V100' # A100 or V100\n",
        "gpu_mem = 32 # 40,32(for V100)\n",
        "rel_thru = 7.76/33.46\n",
        "v100_results = run(pr_list, gpu_type, node_size, gpu_mem, rel_thru)\n",
        "v100_X = []\n",
        "for el in v100_results:\n",
        "    v100_X.append(el[7])"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}